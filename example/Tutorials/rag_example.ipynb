{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG TestSet Generation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up config.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing the RAG based application we have two functionalities, testset generation and RAG evaluation. \n",
    "\n",
    "## Config.ini File\n",
    "\n",
    "Let's define the `config.ini` file with respect to the RAG Testset generation and RAG evaluation. (The complete `config.ini` file and their sample values can be seen [here](https://github.com/michelin/LLMInspector/wiki/Getting-Started)):\n",
    "\n",
    "```ini\n",
    "[RAG_File]\n",
    "RAG_testset_document_directory = Users/input\n",
    "RAG_testset_input_directory = Users/dataset\n",
    "RAG_testset_input_filename = /processed_en_output.xlsx\n",
    "\n",
    "RAG_testset_Output_fileName = /Rag_testset_Output_\n",
    "RAG_eval_Output_fileName = /RAG_eval_Output_\n",
    "RAG_output_directory = Users/output\n",
    "testset_size = 10\n",
    "```\n",
    "\n",
    "RAG_testset_document_directory is the directory that will have all the documents that are used in the RAG. These documents are used to create a testset that has question and ground truth generated based on the documents. Documents can be of the format '.docx' or '.pdf'.\n",
    "\n",
    "RAG_testset_input_directory is the directory that has the excel file, that has the question, ground truth, answer generated by LLM and the context retrieved by the RAG.\n",
    "RAG_testset_input_filename is the name of the excel file.\n",
    "\n",
    "RAG_testset_Output_fileName is the filename that is used for saving the results of the testset generated using the documents.\n",
    "RAG_eval_Output_fileName is the filename that is used for saving the results of the RAG evaluation.\n",
    "\n",
    "Testset size is the total number of questions that will be generated.\n",
    "\n",
    "The RAG testset input file that needs to be evaluated must have four columns question, ground_truth, answer, contexts (in the form of string of lists where each string is retrieved context from RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from llm_inspector.llminspector import llminspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the class object\n",
    "obj = llminspector(config_path=\"config.ini\", env_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.rag_testset_gen()  # Automatically scans the directory containing the documents and saves the excel file of the generated question in the output directory mentioned in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.rag_evaluate()  # Automatically reads the excel file with question, answer, ground_truth, contexts and evaluates them and the output is saved in the output directory as an excel."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
