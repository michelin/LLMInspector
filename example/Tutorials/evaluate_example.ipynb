{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for any chatbot based application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up config.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Insights through evaluation of question, answer and ground truth for accuracy, robustness, sentiment analysis, emotion analysis, translation, language detection, readability scores.\n",
    "\n",
    "## Config.ini File\n",
    "\n",
    "Let's define the `config.ini` file with respect to the Insights. (The complete `config.ini` file and their sample values can be seen [here](https://github.com/michelin/LLMInspector/wiki/Getting-Started)):\n",
    "\n",
    "```ini\n",
    "[Insights_File]\n",
    "Insights_output_path = User/insights_output_file_directory/\n",
    "Insights_Output_fileName = /insights_output_file_name_\n",
    "output_lang = french\n",
    "; fullmetric_list = [\"rouge_score\", \"bert_score\", \"answer_similarity\", \"answer_correctness\", \"question_toxicity\", \"answer_toxicity\", \"pii_detection\", \"readability\", \"translate\", \"question_language\", \"answer_language\", \"question_sentiment\", \"answer_sentiment\", \"question_emotion\", \"answer_emotion\"]\n",
    "Metrics =  [] ; empty list will do all the tests\n",
    "question_col = question\n",
    "answer_col = answer\n",
    "ground_truth_col = ground_truth\n",
    "threshold = 0.9\n",
    "```\n",
    "\n",
    "Insights_output_path and Insights_Output_fileName are the filepath and filename where the insights generated will be saved.\n",
    "\n",
    "Output language is the output language for translation of the generated answer to that respective language.\n",
    "\n",
    "The metrics that are available for testing are [\"rouge_score\", \"bert_score\", \"answer_similarity\", \"answer_correctness\", \"question_toxicity\", \"answer_toxicity\", \"pii_detection\", \"readability\", \"translate\", \"question_language\", \"answer_language\", \"question_sentiment\", \"answer_sentiment\", \"question_emotion\", \"answer_emotion\"]\n",
    "\n",
    "question_col, answer_col, ground_truth_col are the column names of the question, answer and ground truth respectively. \n",
    "\n",
    "threshold is the threshold for the BERTScore if itis used to classify the generated answer as pass or fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from llm_inspector.llminspector import llminspector\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the class object\n",
    "obj = llminspector(config_path=\"config.ini\", env_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Users/input/rag_testset.xlsx\") # Read the excel file that has the question, answer and ground truth.\n",
    "obj.evaluate(df) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
